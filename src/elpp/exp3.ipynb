{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/smaug/ownCloud/praca/reasonable-embeddings/src\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path('../local/out/elpp/')\n",
    "\n",
    "base_dir.mkdir(parents=True, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import lzma\n",
    "import dill\n",
    "\n",
    "with lzma.open(base_dir / 'reasoners.dill.xz', 'rb') as f:\n",
    "    reasoners = dill.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from src.reasoner import ReasonerHead\n",
    "from src.reasoner import EmbeddingLayer\n",
    "\n",
    "with lzma.open(base_dir / 'exp1.dill.xz', 'rb') as f:\n",
    "    artifacts = dill.load(f)\n",
    "\n",
    "emb_size = 10\n",
    "hidden_size = 16\n",
    "\n",
    "for key, components in artifacts.items():\n",
    "    neural_reasoner = ReasonerHead(emb_size=emb_size, hidden_size=hidden_size)\n",
    "    neural_reasoner.load_state_dict(components['reasoner'])\n",
    "    components['reasoner'] = neural_reasoner\n",
    "    encoders = [EmbeddingLayer(emb_size=emb_size, n_concepts=reasoner.n_concepts, n_roles=reasoner.n_roles) for reasoner\n",
    "                in\n",
    "                reasoners]\n",
    "    for sd, e in zip(components['encoders'], encoders):\n",
    "        e.load_state_dict(sd)\n",
    "    components['encoders'] = encoders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "with lzma.open(base_dir / 'test_reasoners.dill.xz', 'rb') as f:\n",
    "    test_reasoners = dill.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 10202 #pos 5101\n",
      "Validation 2528 #pos 1264\n",
      "Test 22480 #pos 22480\n",
      "Training 11504 #pos 5752\n",
      "Validation 2856 #pos 1428\n",
      "Test 21665 #pos 21665\n",
      "Training 12822 #pos 6411\n",
      "Validation 3184 #pos 1592\n",
      "Test 20842 #pos 20842\n",
      "Training 13946 #pos 6973\n",
      "Validation 3464 #pos 1732\n",
      "Test 20140 #pos 20140\n",
      "Training 14814 #pos 7407\n",
      "Validation 3686 #pos 1843\n",
      "Test 19595 #pos 19595\n",
      "Training 15724 #pos 7862\n",
      "Validation 3910 #pos 1955\n",
      "Test 19028 #pos 19028\n",
      "Training 16662 #pos 8331\n",
      "Validation 4146 #pos 2073\n",
      "Test 18441 #pos 18441\n",
      "Training 17546 #pos 8773\n",
      "Validation 4368 #pos 2184\n",
      "Test 17888 #pos 17888\n",
      "Training 18428 #pos 9214\n",
      "Validation 4584 #pos 2292\n",
      "Test 17339 #pos 17339\n",
      "Training 19454 #pos 9727\n",
      "Validation 4844 #pos 2422\n",
      "Test 16696 #pos 16696\n",
      "Training 20449 #pos 10229\n",
      "Validation 5092 #pos 2547\n",
      "Test 16069 #pos 16069\n",
      "Training 21712 #pos 10868\n",
      "Validation 5408 #pos 2707\n",
      "Test 15270 #pos 15270\n",
      "Training 22743 #pos 11460\n",
      "Validation 5664 #pos 2854\n",
      "Test 14531 #pos 14531\n",
      "Training 23756 #pos 12146\n",
      "Validation 5920 #pos 3027\n",
      "Test 13672 #pos 13672\n",
      "Training 24695 #pos 12803\n",
      "Validation 6152 #pos 3190\n",
      "Test 12852 #pos 12852\n",
      "Training 25812 #pos 13581\n",
      "Validation 6437 #pos 3388\n",
      "Test 11876 #pos 11876\n",
      "Training 26486 #pos 14047\n",
      "Validation 6597 #pos 3500\n",
      "Test 11298 #pos 11298\n",
      "Training 26945 #pos 14373\n",
      "Validation 6719 #pos 3585\n",
      "Test 10887 #pos 10887\n",
      "Training 27480 #pos 14734\n",
      "Validation 6847 #pos 3672\n",
      "Test 10439 #pos 10439\n"
     ]
    }
   ],
   "source": [
    "from src.elpp.gen import split_dataset\n",
    "import numpy as np\n",
    "\n",
    "splits = {complexity_threshold_k: split_dataset(test_reasoners, np.random.default_rng(seed=0xbeef),\n",
    "                                                complexity_threshold=complexity_threshold_k) for complexity_threshold_k\n",
    "          in range(2, 21)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 00/15 | batch 319/319 | loss 0.5826 | elapsed 1.77s\n",
      "train epoch 00/15 | batch 360/360 | loss 0.6213 | elapsed 2.24s\n",
      "train epoch 00/15 | batch 319/319 | loss 0.5882 | elapsed 2.18s\n",
      "train epoch 01/15 | batch 319/319 | loss 0.5742 | elapsed 4.08s\n",
      "train epoch 00/15 | batch 360/360 | loss 0.6279 | elapsed 2.12s\n",
      "train epoch 01/15 | batch 319/319 | loss 0.5803 | elapsed 3.20s\n",
      "train epoch 01/15 | batch 360/360 | loss 0.6079 | elapsed 4.26s\n",
      "train epoch 02/15 | batch 319/319 | loss 0.5416 | elapsed 3.21s\n",
      "train epoch 01/15 | batch 360/360 | loss 0.6152 | elapsed 3.68s\n",
      "train epoch 02/15 | batch 319/319 | loss 0.5474 | elapsed 3.72s\n",
      "train epoch 03/15 | batch 319/319 | loss 0.5114 | elapsed 3.60s\n",
      "train epoch 02/15 | batch 360/360 | loss 0.5670 | elapsed 4.08s\n",
      "train epoch 03/15 | batch 319/319 | loss 0.5164 | elapsed 3.33s\n",
      "train epoch 02/15 | batch 360/360 | loss 0.5729 | elapsed 4.90s\n",
      "train epoch 04/15 | batch 319/319 | loss 0.4846 | elapsed 3.45s\n",
      "train epoch 03/15 | batch 360/360 | loss 0.5301 | elapsed 3.78s\n",
      "train epoch 04/15 | batch 319/319 | loss 0.4886 | elapsed 3.23s\n",
      "train epoch 05/15 | batch 319/319 | loss 0.4609 | elapsed 3.33s\n",
      "train epoch 04/15 | batch 360/360 | loss 0.4987 | elapsed 3.79s\n",
      "train epoch 03/15 | batch 360/360 | loss 0.5351 | elapsed 3.65s\n",
      "train epoch 05/15 | batch 319/319 | loss 0.4636 | elapsed 3.50s\n",
      "train epoch 04/15 | batch 360/360 | loss 0.5018 | elapsed 3.89s\n",
      "train epoch 06/15 | batch 319/319 | loss 0.4395 | elapsed 3.67s\n",
      "train epoch 05/15 | batch 360/360 | loss 0.4701 | elapsed 4.05s\n",
      "train epoch 06/15 | batch 319/319 | loss 0.4409 | elapsed 3.44s\n",
      "train epoch 07/15 | batch 319/319 | loss 0.4199 | elapsed 3.51s\n",
      "train epoch 05/15 | batch 360/360 | loss 0.4715 | elapsed 3.90s\n",
      "train epoch 06/15 | batch 360/360 | loss 0.4457 | elapsed 3.95s\n",
      "train epoch 07/15 | batch 319/319 | loss 0.4199 | elapsed 3.35s\n",
      "train epoch 08/15 | batch 319/319 | loss 0.4022 | elapsed 3.38s\n",
      "train epoch 06/15 | batch 360/360 | loss 0.4453 | elapsed 3.81s\n",
      "train epoch 07/15 | batch 360/360 | loss 0.4231 | elapsed 3.60s\n",
      "train epoch 08/15 | batch 319/319 | loss 0.4008 | elapsed 3.10s\n",
      "train epoch 09/15 | batch 319/319 | loss 0.3858 | elapsed 3.13s\n",
      "train epoch 07/15 | batch 360/360 | loss 0.4205 | elapsed 3.61s\n",
      "train epoch 09/15 | batch 319/319 | loss 0.3830 | elapsed 3.12s\n",
      "train epoch 08/15 | batch 360/360 | loss 0.4030 | elapsed 3.53s\n",
      "train epoch 10/15 | batch 319/319 | loss 0.3705 | elapsed 3.33s\n",
      "train epoch 08/15 | batch 360/360 | loss 0.3988 | elapsed 3.82s\n",
      "train epoch 09/15 | batch 360/360 | loss 0.3850 | elapsed 4.36s\n",
      "train epoch 10/15 | batch 319/319 | loss 0.3665 | elapsed 3.45s\n",
      "train epoch 11/15 | batch 319/319 | loss 0.3562 | elapsed 4.12s\n",
      "train epoch 09/15 | batch 360/360 | loss 0.3790 | elapsed 4.57s\n",
      "train epoch 11/15 | batch 319/319 | loss 0.3511 | elapsed 4.28s\n",
      "train epoch 10/15 | batch 360/360 | loss 0.3677 | elapsed 4.34s\n",
      "train epoch 12/15 | batch 319/319 | loss 0.3429 | elapsed 4.31s\n",
      "train epoch 10/15 | batch 360/360 | loss 0.3603 | elapsed 4.46s\n",
      "train epoch 12/15 | batch 319/319 | loss 0.3367 | elapsed 3.77s\n",
      "train epoch 11/15 | batch 360/360 | loss 0.3522 | elapsed 4.24s\n",
      "train epoch 13/15 | batch 319/319 | loss 0.3301 | elapsed 3.57s\n",
      "train epoch 13/15 | batch 319/319 | loss 0.3230 | elapsed 3.57s\n",
      "train epoch 11/15 | batch 360/360 | loss 0.3433 | elapsed 3.96s\n",
      "train epoch 12/15 | batch 360/360 | loss 0.3375 | elapsed 4.08s\n",
      "train epoch 14/15 | batch 319/319 | loss 0.3183 | elapsed 3.62s\n",
      "train epoch 14/15 | batch 319/319 | loss 0.3102 | elapsed 3.45s\n",
      "train epoch 12/15 | batch 360/360 | loss 0.3274 | elapsed 4.03s\n",
      "train epoch 13/15 | batch 360/360 | loss 0.3241 | elapsed 4.10s\n",
      "train epoch 13/15 | batch 360/360 | loss 0.3129 | elapsed 4.12s\n",
      "train epoch 14/15 | batch 360/360 | loss 0.3114 | elapsed 4.15s\n",
      "train epoch 14/15 | batch 360/360 | loss 0.2992 | elapsed 3.68s\n",
      "train epoch 15/15 | batch 261/360 | loss 0.2846 | elapsed 2.52s"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed, cpu_count\n",
    "from src.reasoner import EmbeddingLayer, train, eval_batch\n",
    "from src.utils import timestr, paramcount\n",
    "import torch as T\n",
    "\n",
    "seed = 2022\n",
    "ts = timestr()\n",
    "\n",
    "emb_size = 10\n",
    "hidden_size = 16\n",
    "epoch_count = 15\n",
    "test_epoch_count = 10\n",
    "batch_size = 32\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "\n",
    "def train_helper(complexity_threshold_j, complexity_threshold_k):\n",
    "    neural_reasoner = artifacts[complexity_threshold_j][\"reasoner\"]\n",
    "    training, validation, test = splits[complexity_threshold_k]\n",
    "    T.manual_seed(seed)\n",
    "    my_encoders = [EmbeddingLayer(emb_size=emb_size, n_concepts=reasoner.n_concepts, n_roles=reasoner.n_roles) for\n",
    "                   reasoner in test_reasoners]\n",
    "\n",
    "    train_logger = train(training, validation, neural_reasoner, my_encoders, epoch_count=epoch_count,\n",
    "                         batch_size=batch_size,\n",
    "                         freeze_reasoner=True, validate=False)\n",
    "\n",
    "    with T.no_grad():\n",
    "        idx_te, X_te, y_te = test\n",
    "        _, _, Y_te_good = eval_batch(neural_reasoner, my_encoders, X_te, y_te, idx_te)\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(idx_te)):\n",
    "        idx = idx_te[i]\n",
    "        axiom = X_te[i]\n",
    "        expected = y_te[i]\n",
    "        predicted = Y_te_good[i]\n",
    "        complexity = len(test_reasoners[idx].decode_shortest_proof(axiom[1], axiom[2]))\n",
    "        rows.append([complexity_threshold_j, complexity_threshold_k, idx, complexity, axiom, expected, int(predicted >= .5), predicted])\n",
    "    print(f\"({complexity_threshold_j}, {complexity_threshold_k}) completed\")\n",
    "\n",
    "    return complexity_threshold_j, complexity_threshold_k, my_encoders, rows\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(train_helper)(complexity_threshold_j, complexity_threshold_k) for complexity_threshold_j in range(2, 21) for\n",
    "    complexity_threshold_k in range(2, 21))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "encoders = {}\n",
    "rows = []\n",
    "\n",
    "for j, k, jk_encoders, some_rows in results:\n",
    "    encoders[(j, k)] = jk_encoders\n",
    "    rows += some_rows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "tmp = {\n",
    "    'splits': splits,\n",
    "    'encoders': {key: [e.state_dict() for e in encs] for key, encs in encoders.items()}\n",
    "}\n",
    "\n",
    "with lzma.open(base_dir / 'exp3.dill.xz', 'wb') as f:\n",
    "    dill.dump(tmp, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "       Complexity threshold j  Complexity threshold k  KB  Complexity  \\\n0                           2                       2   0           9   \n1                           2                       2   0           3   \n2                           2                       2   0           8   \n3                           2                       2   0          10   \n4                           2                       2   0           7   \n...                       ...                     ...  ..         ...   \n88285                       3                       3  19           6   \n88286                       3                       3  19           8   \n88287                       3                       3  19           5   \n88288                       3                       3  19           9   \n88289                       3                       3  19           4   \n\n             Axiom  Expected  Predicted  Raw predicted  \n0       (0, 4, 53)         1          1       0.566736  \n1       (0, 7, 53)         1          1       0.508733  \n2        (0, 9, 0)         1          1       0.591396  \n3       (0, 9, 46)         1          0       0.222073  \n4       (0, 9, 53)         1          0       0.499289  \n...            ...       ...        ...            ...  \n88285  (0, 95, 80)         1          0       0.400699  \n88286  (0, 95, 85)         1          1       0.659106  \n88287  (0, 95, 87)         1          1       0.525250  \n88288  (0, 95, 89)         1          1       0.908568  \n88289  (0, 95, 98)         1          0       0.452733  \n\n[88290 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Complexity threshold j</th>\n      <th>Complexity threshold k</th>\n      <th>KB</th>\n      <th>Complexity</th>\n      <th>Axiom</th>\n      <th>Expected</th>\n      <th>Predicted</th>\n      <th>Raw predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>9</td>\n      <td>(0, 4, 53)</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.566736</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>(0, 7, 53)</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.508733</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>8</td>\n      <td>(0, 9, 0)</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.591396</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>10</td>\n      <td>(0, 9, 46)</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.222073</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>7</td>\n      <td>(0, 9, 53)</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.499289</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>88285</th>\n      <td>3</td>\n      <td>3</td>\n      <td>19</td>\n      <td>6</td>\n      <td>(0, 95, 80)</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.400699</td>\n    </tr>\n    <tr>\n      <th>88286</th>\n      <td>3</td>\n      <td>3</td>\n      <td>19</td>\n      <td>8</td>\n      <td>(0, 95, 85)</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.659106</td>\n    </tr>\n    <tr>\n      <th>88287</th>\n      <td>3</td>\n      <td>3</td>\n      <td>19</td>\n      <td>5</td>\n      <td>(0, 95, 87)</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.525250</td>\n    </tr>\n    <tr>\n      <th>88288</th>\n      <td>3</td>\n      <td>3</td>\n      <td>19</td>\n      <td>9</td>\n      <td>(0, 95, 89)</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.908568</td>\n    </tr>\n    <tr>\n      <th>88289</th>\n      <td>3</td>\n      <td>3</td>\n      <td>19</td>\n      <td>4</td>\n      <td>(0, 95, 98)</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.452733</td>\n    </tr>\n  </tbody>\n</table>\n<p>88290 rows Ã— 8 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"Complexity threshold j\",\"Complexity threshold k\", \"KB\", \"Complexity\", \"Axiom\", \"Expected\", \"Predicted\",\n",
    "                                 \"Raw predicted\"])\n",
    "df.to_feather(base_dir / 'exp3.feather')\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "reasonable-embeddings",
   "language": "python",
   "display_name": "reasonable-embeddings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
